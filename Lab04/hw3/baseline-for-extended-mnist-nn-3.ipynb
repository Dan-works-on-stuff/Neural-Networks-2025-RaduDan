{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 117046,
     "databundleVersionId": 13980102,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import dtype, ndarray\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-05T15:20:18.100656Z",
     "iopub.execute_input": "2025-10-05T15:20:18.101004Z",
     "iopub.status.idle": "2025-10-05T15:20:18.106379Z",
     "shell.execute_reply.started": "2025-10-05T15:20:18.100978Z",
     "shell.execute_reply": "2025-10-05T15:20:18.104988Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-11T10:26:19.045581200Z",
     "start_time": "2025-11-11T10:26:18.615742600Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "train_file = \"extended_mnist_train.pkl\"\n",
    "test_file = \"extended_mnist_test.pkl\"\n",
    "\n",
    "with open(train_file, \"rb\") as fp:\n",
    "    train = pickle.load(fp)\n",
    "\n",
    "with open(test_file, \"rb\") as fp:\n",
    "    test = pickle.load(fp)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-05T15:20:18.107853Z",
     "iopub.execute_input": "2025-10-05T15:20:18.108515Z",
     "iopub.status.idle": "2025-10-05T15:20:18.469405Z",
     "shell.execute_reply.started": "2025-10-05T15:20:18.108486Z",
     "shell.execute_reply": "2025-10-05T15:20:18.46824Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-11T10:26:20.273600100Z",
     "start_time": "2025-11-11T10:26:20.128451900Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": "train_data = []\ntrain_labels = []\nfor image, label in train:\n    train_data.append(image.flatten())\n    train_labels.append(label)\n",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-05T15:20:18.471547Z",
     "iopub.execute_input": "2025-10-05T15:20:18.471825Z",
     "iopub.status.idle": "2025-10-05T15:20:18.507104Z",
     "shell.execute_reply.started": "2025-10-05T15:20:18.471805Z",
     "shell.execute_reply": "2025-10-05T15:20:18.506027Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-11T10:26:21.507141200Z",
     "start_time": "2025-11-11T10:26:21.441880800Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": "test_data = []\nfor image, label in test:\n    test_data.append(image.flatten())\n",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-05T15:20:18.509387Z",
     "iopub.execute_input": "2025-10-05T15:20:18.509987Z",
     "iopub.status.idle": "2025-10-05T15:20:18.519545Z",
     "shell.execute_reply.started": "2025-10-05T15:20:18.509954Z",
     "shell.execute_reply": "2025-10-05T15:20:18.518525Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-11T10:26:22.790091800Z",
     "start_time": "2025-11-11T10:26:22.771155900Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "# You must use NumPy to implement from scratch\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "class MLP:\n",
    "\n",
    "    def __init__(self, input_dim=784, hidden_dim=100, output_dim=10, weight_decay=1e-4, seed=42):\n",
    "        \"\"\"\n",
    "        Initializes the network's weights and biases.\n",
    "        \"\"\"\n",
    "        rng = np.random.default_rng(seed)\n",
    "\n",
    "        # Layer 1: Input to Hidden Layer\n",
    "        # Using He initialization, which is well-suited for ReLU activation functions.\n",
    "        self.WeightMatrix1 = rng.normal(0.0, np.sqrt(2.0 / input_dim), size=(input_dim, hidden_dim)).astype(np.float32)\n",
    "        self.BiasVectorInput = np.zeros((hidden_dim,), dtype=np.float32)\n",
    "\n",
    "        # Layer 2: Hidden Layer to Output Layer\n",
    "        \n",
    "        self.WeightMatrix2 = rng.normal(0.0, np.sqrt(2.0 / hidden_dim), size=(hidden_dim, output_dim)).astype(np.float32)\n",
    "        self.BiasVectorOutput = np.zeros((output_dim,), dtype=np.float32)\n",
    "\n",
    "        self.weight_decay = float(weight_decay)  # Weight decay coefficient for L2 regularization\n",
    "\n",
    "    @staticmethod\n",
    "    def _preprocess(X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Normalizes pixel values from the 0-255 range to the 0.0-1.0 range.\n",
    "        \"\"\"\n",
    "        X_float = np.asarray(X, dtype=np.float32)\n",
    "        return X_float / 255.0\n",
    "\n",
    "    # ReLU: f(x) = max(0, x)\n",
    "    @staticmethod\n",
    "    def _relu(z: np.ndarray) -> np.ndarray:\n",
    "        return np.maximum(z, 0.0, dtype=np.float32)\n",
    "\n",
    "    @staticmethod\n",
    "    def _relu_grad(z: np.ndarray) -> np.ndarray:\n",
    "        # The gradient is 1 for inputs > 0, and 0 otherwise.\n",
    "        grad = np.zeros_like(z, dtype=np.float32)\n",
    "        grad[z > 0] = 1.0\n",
    "        return grad\n",
    "\n",
    "    @staticmethod\n",
    "    def _softmax(logits: np.ndarray) -> np.ndarray: # converts logits into softmaxProbabilities.\n",
    "        stabilizederivativeLossLogits = logits - logits.max(axis=1, keepdims=True)\n",
    "        # Use higher precision for the exponentiation step\n",
    "        exponentials = np.exp(stabilizederivativeLossLogits, dtype=np.float64)\n",
    "        # Normalize to get softmaxProbabilities\n",
    "        softmaxProbabilities = exponentials / exponentials.sum(axis=1, keepdims=True)\n",
    "        return softmaxProbabilities.astype(np.float32)\n",
    "\n",
    "    def forward(self, X: np.ndarray, cache: bool = False) -> tuple[ndarray[tuple[Any, ...], dtype[Any]], dict[\n",
    "        str, ndarray[tuple[Any, ...], dtype[Any]] | ndarray[tuple[Any, ...], dtype[Any]]]] | tuple[\n",
    "                                                                 ndarray[tuple[Any, ...], dtype[Any]], None]:\n",
    "        X_normalized = self._preprocess(X)\n",
    "\n",
    "        z1 = X_normalized @ self.WeightMatrix1 + self.BiasVectorInput\n",
    "        h1 = self._relu(z1)\n",
    "\n",
    "        logits = h1 @ self.WeightMatrix2 + self.BiasVectorOutput\n",
    "\n",
    "        if cache:\n",
    "            saved_cache = {\"X_normalized\": X_normalized, \"z1\": z1, \"h1\": h1}\n",
    "            return logits, saved_cache\n",
    "        return logits, None\n",
    "\n",
    "    def _loss_and_grads(self, X: np.ndarray, y: np.ndarray) -> Tuple[float, Dict[str, np.ndarray]]:\n",
    "        # Get the final scores (logits) and the intermediate values\n",
    "        logits, cache = self.forward(X, cache=True)\n",
    "        num_samples = y.shape[0]\n",
    "\n",
    "        # --- Loss Calculation ---\n",
    "        softmaxProbabilities = self._softmax(logits)\n",
    "\n",
    "        # log-likelihood loss\n",
    "        correct_logsoftmaxProbabilities = -np.log(np.clip(softmaxProbabilities[np.arange(num_samples), y], 1e-12, 1.0))\n",
    "        data_loss = float(correct_logsoftmaxProbabilities.mean())\n",
    "\n",
    "        # L2\n",
    "        regularizationLoss = 0.5 * self.weight_decay * (np.sum(self.WeightMatrix1**2) + np.sum(self.WeightMatrix2**2)) # a penalty to prevent overfitting\n",
    "        total_loss = data_loss + float(regularizationLoss)\n",
    "\n",
    "        # Backpropagation\n",
    "        # Gradient of the loss\n",
    "        derivativeLossLogits = softmaxProbabilities\n",
    "        derivativeLossLogits[np.arange(num_samples), y] -= 1.0\n",
    "        derivativeLossLogits /= num_samples\n",
    "\n",
    "        # Gradients for WeightMatrix2 and BiasVectorOutput (Output Layer)\n",
    "        d_WeightMatrix2 = cache[\"h1\"].T @ derivativeLossLogits + self.weight_decay * self.WeightMatrix2\n",
    "        d_BiasVectorOutput = derivativeLossLogits.sum(axis=0)\n",
    "\n",
    "        # gradient back to the hidden layer\n",
    "        derivativeHiddenLayer1 = derivativeLossLogits @ self.WeightMatrix2.T\n",
    "        lossGradientBeforeReLU = derivativeHiddenLayer1 * self._relu_grad(cache[\"z1\"])   # chain rule\n",
    "\n",
    "        # gradients for WeightMatrix1 and BiasVectorInput (Hidden Layer)\n",
    "        d_WeightMatrix1 = cache[\"X_normalized\"].T @ lossGradientBeforeReLU + self.weight_decay * self.WeightMatrix1\n",
    "        d_BiasVectorInput = lossGradientBeforeReLU.sum(axis=0)\n",
    "\n",
    "        # a dictionary for gradients\n",
    "        grads = {\"WeightMatrix1\": d_WeightMatrix1, \"BiasVectorInput\": d_BiasVectorInput, \"WeightMatrix2\": d_WeightMatrix2, \"BiasVectorOutput\": d_BiasVectorOutput}\n",
    "        return total_loss, grads\n",
    "\n",
    "    def _accuracy(self, X: np.ndarray, y: np.ndarray, batch_size: int = 1024) -> float:\n",
    "        predictions = self.predict(X, batch_size=batch_size)\n",
    "        return float((predictions == y).mean())\n",
    "\n",
    "    def evaluate(self, X: np.ndarray, y: np.ndarray, batch_size: int = 1024) -> Tuple[float, float]:\n",
    "        num_samples = X.shape[0]\n",
    "        total_data_loss = 0.0\n",
    "\n",
    "        # calculates loss\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            X_batch = X[i:i + batch_size]\n",
    "            y_batch = y[i:i + batch_size]\n",
    "            num_batch_samples = y_batch.shape[0]\n",
    "\n",
    "            logits, _ = self.forward(X_batch, cache=False)\n",
    "            softmaxProbabilities = self._softmax(logits)\n",
    "            correct_logsoftmaxProbabilities = -np.log(np.clip(softmaxProbabilities[np.arange(num_batch_samples), y_batch], 1e-12, 1.0))\n",
    "            total_data_loss += correct_logsoftmaxProbabilities.sum()\n",
    "\n",
    "        # average data loss + regularization loss\n",
    "        mean_data_loss = total_data_loss / num_samples\n",
    "        regularizationLoss = 0.5 * self.weight_decay * (np.sum(self.WeightMatrix1**2) + np.sum(self.WeightMatrix2**2))\n",
    "        total_loss = mean_data_loss + regularizationLoss\n",
    "\n",
    "        # Calculate accuracy\n",
    "        acc = self._accuracy(X, y, batch_size=batch_size)\n",
    "        return float(total_loss), acc\n",
    "\n",
    "    def fit(self,\n",
    "            X_train: np.ndarray,\n",
    "            y_train: np.ndarray,\n",
    "            X_val: np.ndarray,\n",
    "            y_val: np.ndarray,\n",
    "            epochs: int = 20,\n",
    "            batch_size: int = 256,\n",
    "            lr: float = 0.1,\n",
    "            seed: int = 42) -> Dict[str, list]:\n",
    "\n",
    "        # Trains the model using mini-batch Stochastic Gradient Descent (SGD).\n",
    "\n",
    "        rng = np.random.default_rng(seed)\n",
    "        X_train = np.asarray(X_train)\n",
    "        y_train = np.asarray(y_train, dtype=np.int64)\n",
    "        X_val = np.asarray(X_val)\n",
    "        y_val = np.asarray(y_val, dtype=np.int64)\n",
    "\n",
    "        history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            # Shuffle the training data at the beginning of each epoch\n",
    "            shuffled_indices = rng.permutation(X_train.shape[0])\n",
    "            X_train_shuffled = X_train[shuffled_indices]\n",
    "            y_train_shuffled = y_train[shuffled_indices]\n",
    "\n",
    "            # Process the data in mini-batches\n",
    "            for i in range(0, X_train.shape[0], batch_size):\n",
    "                X_batch = X_train_shuffled[i:i + batch_size]\n",
    "                y_batch = y_train_shuffled[i:i + batch_size]\n",
    "\n",
    "                loss, grads = self._loss_and_grads(X_batch, y_batch)\n",
    "\n",
    "                # Update parameters using SGD\n",
    "                self.WeightMatrix1 -= lr * grads[\"WeightMatrix1\"]\n",
    "                self.BiasVectorInput -= lr * grads[\"BiasVectorInput\"]\n",
    "                self.WeightMatrix2 -= lr * grads[\"WeightMatrix2\"]\n",
    "                self.BiasVectorOutput -= lr * grads[\"BiasVectorOutput\"]\n",
    "\n",
    "            # Evaluate and record metrics at the end of the epoch\n",
    "            tr_loss, tr_acc = self.evaluate(X_train, y_train)\n",
    "            va_loss, va_acc = self.evaluate(X_val, y_val)\n",
    "            history[\"train_loss\"].append(tr_loss)\n",
    "            history[\"train_acc\"].append(tr_acc)\n",
    "            history[\"val_loss\"].append(va_loss)\n",
    "            history[\"val_acc\"].append(va_acc)\n",
    "            print(f\"epoch {epoch:02d} | train loss {tr_loss:.4f} acc {tr_acc:.4f} | val loss {va_loss:.4f} acc {va_acc:.4f}\")\n",
    "\n",
    "        return history\n",
    "\n",
    "    def predict(self, X: np.ndarray, batch_size: int = 1024) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Makes predictions for a given dataset.\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        num_samples = X.shape[0]\n",
    "        predictions = np.empty((num_samples,), dtype=np.int64)\n",
    "\n",
    "        # Process in batches to conserve memory\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            X_batch = X[i:i + batch_size]\n",
    "            logits, _ = self.forward(X_batch, cache=False)\n",
    "            predictions[i:i + batch_size] = np.argmax(logits, axis=1)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "# --- Training and evaluation (place this cell before the existing 'predictions = mlp.predict(test_data)' cell) ---\n",
    "\n",
    "# Use the prepared lists from the notebook: train_data, train_labels\n",
    "X = np.asarray(train_data, dtype=np.float32)\n",
    "y = np.asarray(train_labels, dtype=np.int64)\n",
    "\n",
    "# Train/validation split\n",
    "rng = np.random.default_rng(123)\n",
    "perm = rng.permutation(X.shape[0])\n",
    "val_ratio = 0.1\n",
    "val_size = int(len(perm) * val_ratio)\n",
    "val_idx = perm[:val_size]\n",
    "train_idx = perm[val_size:]\n",
    "\n",
    "X_tr, y_tr = X[train_idx], y[train_idx]\n",
    "X_va, y_va = X[val_idx], y[val_idx]\n",
    "\n",
    "# Model, train, and report\n",
    "# Using the new, slower, more readable model with 100 hidden neurons\n",
    "mlp = MLP(input_dim=784, hidden_dim=100, output_dim=10, weight_decay=5e-4, seed=1)\n",
    "history = mlp.fit(X_tr, y_tr, X_va, y_va, epochs=80, batch_size=256, lr=0.1, seed=1)\n",
    "\n",
    "# Final metrics\n",
    "final_train_loss, final_train_acc = mlp.evaluate(X_tr, y_tr)\n",
    "final_val_loss, final_val_acc = mlp.evaluate(X_va, y_va)\n",
    "print(f\"final train: loss={final_train_loss:.4f}, acc={final_train_acc:.4f}\")\n",
    "print(f\"final val:   loss={final_val_loss:.4f}, acc={final_val_acc:.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-05T15:20:18.520758Z",
     "iopub.execute_input": "2025-10-05T15:20:18.521683Z",
     "iopub.status.idle": "2025-10-05T15:20:26.682486Z",
     "shell.execute_reply.started": "2025-10-05T15:20:18.521652Z",
     "shell.execute_reply": "2025-10-05T15:20:26.681373Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-11T10:27:05.215885900Z",
     "start_time": "2025-11-11T10:26:23.888666700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01 | train loss 0.4188 acc 0.9012 | val loss 0.4344 acc 0.8977\n",
      "epoch 02 | train loss 0.3562 acc 0.9165 | val loss 0.3752 acc 0.9127\n",
      "epoch 03 | train loss 0.3214 acc 0.9276 | val loss 0.3447 acc 0.9225\n",
      "epoch 04 | train loss 0.2966 acc 0.9345 | val loss 0.3226 acc 0.9262\n",
      "epoch 05 | train loss 0.2786 acc 0.9398 | val loss 0.3067 acc 0.9322\n",
      "epoch 06 | train loss 0.2617 acc 0.9445 | val loss 0.2898 acc 0.9375\n",
      "epoch 07 | train loss 0.2512 acc 0.9487 | val loss 0.2818 acc 0.9405\n",
      "epoch 08 | train loss 0.2417 acc 0.9501 | val loss 0.2736 acc 0.9420\n",
      "epoch 09 | train loss 0.2283 acc 0.9551 | val loss 0.2608 acc 0.9465\n",
      "epoch 10 | train loss 0.2216 acc 0.9568 | val loss 0.2546 acc 0.9452\n",
      "epoch 11 | train loss 0.2133 acc 0.9599 | val loss 0.2467 acc 0.9480\n",
      "epoch 12 | train loss 0.2069 acc 0.9617 | val loss 0.2413 acc 0.9513\n",
      "epoch 13 | train loss 0.2006 acc 0.9630 | val loss 0.2362 acc 0.9500\n",
      "epoch 14 | train loss 0.1950 acc 0.9646 | val loss 0.2325 acc 0.9513\n",
      "epoch 15 | train loss 0.1921 acc 0.9664 | val loss 0.2283 acc 0.9517\n",
      "epoch 16 | train loss 0.1867 acc 0.9672 | val loss 0.2254 acc 0.9545\n",
      "epoch 17 | train loss 0.1809 acc 0.9699 | val loss 0.2186 acc 0.9560\n",
      "epoch 18 | train loss 0.1782 acc 0.9706 | val loss 0.2150 acc 0.9575\n",
      "epoch 19 | train loss 0.1735 acc 0.9719 | val loss 0.2126 acc 0.9573\n",
      "epoch 20 | train loss 0.1719 acc 0.9727 | val loss 0.2130 acc 0.9577\n",
      "epoch 21 | train loss 0.1682 acc 0.9736 | val loss 0.2098 acc 0.9585\n",
      "epoch 22 | train loss 0.1649 acc 0.9747 | val loss 0.2063 acc 0.9583\n",
      "epoch 23 | train loss 0.1621 acc 0.9754 | val loss 0.2042 acc 0.9585\n",
      "epoch 24 | train loss 0.1612 acc 0.9753 | val loss 0.2041 acc 0.9590\n",
      "epoch 25 | train loss 0.1574 acc 0.9769 | val loss 0.1998 acc 0.9607\n",
      "epoch 26 | train loss 0.1557 acc 0.9774 | val loss 0.1986 acc 0.9608\n",
      "epoch 27 | train loss 0.1531 acc 0.9779 | val loss 0.1961 acc 0.9618\n",
      "epoch 28 | train loss 0.1518 acc 0.9782 | val loss 0.1953 acc 0.9622\n",
      "epoch 29 | train loss 0.1497 acc 0.9790 | val loss 0.1924 acc 0.9623\n",
      "epoch 30 | train loss 0.1479 acc 0.9796 | val loss 0.1915 acc 0.9633\n",
      "epoch 31 | train loss 0.1465 acc 0.9799 | val loss 0.1913 acc 0.9627\n",
      "epoch 32 | train loss 0.1448 acc 0.9804 | val loss 0.1893 acc 0.9630\n",
      "epoch 33 | train loss 0.1436 acc 0.9812 | val loss 0.1892 acc 0.9637\n",
      "epoch 34 | train loss 0.1424 acc 0.9808 | val loss 0.1885 acc 0.9627\n",
      "epoch 35 | train loss 0.1410 acc 0.9812 | val loss 0.1863 acc 0.9638\n",
      "epoch 36 | train loss 0.1394 acc 0.9821 | val loss 0.1854 acc 0.9638\n",
      "epoch 37 | train loss 0.1388 acc 0.9823 | val loss 0.1831 acc 0.9650\n",
      "epoch 38 | train loss 0.1371 acc 0.9827 | val loss 0.1831 acc 0.9642\n",
      "epoch 39 | train loss 0.1360 acc 0.9828 | val loss 0.1831 acc 0.9652\n",
      "epoch 40 | train loss 0.1352 acc 0.9834 | val loss 0.1801 acc 0.9658\n",
      "epoch 41 | train loss 0.1337 acc 0.9842 | val loss 0.1800 acc 0.9655\n",
      "epoch 42 | train loss 0.1325 acc 0.9840 | val loss 0.1801 acc 0.9660\n",
      "epoch 43 | train loss 0.1320 acc 0.9846 | val loss 0.1788 acc 0.9650\n",
      "epoch 44 | train loss 0.1308 acc 0.9848 | val loss 0.1783 acc 0.9667\n",
      "epoch 45 | train loss 0.1306 acc 0.9849 | val loss 0.1790 acc 0.9660\n",
      "epoch 46 | train loss 0.1296 acc 0.9854 | val loss 0.1779 acc 0.9658\n",
      "epoch 47 | train loss 0.1285 acc 0.9851 | val loss 0.1768 acc 0.9663\n",
      "epoch 48 | train loss 0.1275 acc 0.9857 | val loss 0.1745 acc 0.9673\n",
      "epoch 49 | train loss 0.1265 acc 0.9857 | val loss 0.1734 acc 0.9677\n",
      "epoch 50 | train loss 0.1269 acc 0.9861 | val loss 0.1750 acc 0.9683\n",
      "epoch 51 | train loss 0.1252 acc 0.9864 | val loss 0.1740 acc 0.9675\n",
      "epoch 52 | train loss 0.1250 acc 0.9866 | val loss 0.1714 acc 0.9692\n",
      "epoch 53 | train loss 0.1243 acc 0.9866 | val loss 0.1725 acc 0.9675\n",
      "epoch 54 | train loss 0.1236 acc 0.9864 | val loss 0.1729 acc 0.9683\n",
      "epoch 55 | train loss 0.1225 acc 0.9873 | val loss 0.1711 acc 0.9693\n",
      "epoch 56 | train loss 0.1228 acc 0.9866 | val loss 0.1723 acc 0.9672\n",
      "epoch 57 | train loss 0.1212 acc 0.9878 | val loss 0.1705 acc 0.9678\n",
      "epoch 58 | train loss 0.1208 acc 0.9876 | val loss 0.1684 acc 0.9703\n",
      "epoch 59 | train loss 0.1204 acc 0.9876 | val loss 0.1698 acc 0.9690\n",
      "epoch 60 | train loss 0.1195 acc 0.9881 | val loss 0.1681 acc 0.9690\n",
      "epoch 61 | train loss 0.1189 acc 0.9881 | val loss 0.1683 acc 0.9683\n",
      "epoch 62 | train loss 0.1192 acc 0.9881 | val loss 0.1682 acc 0.9692\n",
      "epoch 63 | train loss 0.1181 acc 0.9884 | val loss 0.1670 acc 0.9703\n",
      "epoch 64 | train loss 0.1176 acc 0.9884 | val loss 0.1671 acc 0.9682\n",
      "epoch 65 | train loss 0.1168 acc 0.9888 | val loss 0.1661 acc 0.9693\n",
      "epoch 66 | train loss 0.1163 acc 0.9888 | val loss 0.1662 acc 0.9692\n",
      "epoch 67 | train loss 0.1163 acc 0.9890 | val loss 0.1649 acc 0.9705\n",
      "epoch 68 | train loss 0.1166 acc 0.9891 | val loss 0.1661 acc 0.9683\n",
      "epoch 69 | train loss 0.1155 acc 0.9891 | val loss 0.1652 acc 0.9687\n",
      "epoch 70 | train loss 0.1148 acc 0.9896 | val loss 0.1648 acc 0.9702\n",
      "epoch 71 | train loss 0.1152 acc 0.9891 | val loss 0.1653 acc 0.9685\n",
      "epoch 72 | train loss 0.1140 acc 0.9897 | val loss 0.1638 acc 0.9697\n",
      "epoch 73 | train loss 0.1145 acc 0.9897 | val loss 0.1641 acc 0.9685\n",
      "epoch 74 | train loss 0.1136 acc 0.9899 | val loss 0.1636 acc 0.9687\n",
      "epoch 75 | train loss 0.1128 acc 0.9901 | val loss 0.1627 acc 0.9698\n",
      "epoch 76 | train loss 0.1131 acc 0.9901 | val loss 0.1635 acc 0.9698\n",
      "epoch 77 | train loss 0.1131 acc 0.9899 | val loss 0.1640 acc 0.9705\n",
      "epoch 78 | train loss 0.1120 acc 0.9905 | val loss 0.1623 acc 0.9698\n",
      "epoch 79 | train loss 0.1125 acc 0.9904 | val loss 0.1638 acc 0.9682\n",
      "epoch 80 | train loss 0.1116 acc 0.9906 | val loss 0.1628 acc 0.9685\n",
      "final train: loss=0.1116, acc=0.9906\n",
      "final val:   loss=0.1628, acc=0.9685\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": "predictions = mlp.predict(test_data)\n",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-05T15:20:26.683571Z",
     "iopub.execute_input": "2025-10-05T15:20:26.684022Z",
     "iopub.status.idle": "2025-10-05T15:20:26.736698Z",
     "shell.execute_reply.started": "2025-10-05T15:20:26.683994Z",
     "shell.execute_reply": "2025-10-05T15:20:26.735476Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-10T17:00:47.283622Z",
     "start_time": "2025-11-10T17:00:47.252909Z"
    }
   },
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "source": "# This is how you prepare a submission for the competition\npredictions_csv = {\n    \"ID\": [],\n    \"target\": [],\n}\n\nfor i, label in enumerate(predictions):\n    predictions_csv[\"ID\"].append(i)\n    predictions_csv[\"target\"].append(label)\n\ndf = pd.DataFrame(predictions_csv)\ndf.to_csv(\"submission.csv\", index=False)",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-05T15:20:26.737946Z",
     "iopub.execute_input": "2025-10-05T15:20:26.738456Z",
     "iopub.status.idle": "2025-10-05T15:20:26.78816Z",
     "shell.execute_reply.started": "2025-10-05T15:20:26.738425Z",
     "shell.execute_reply": "2025-10-05T15:20:26.787109Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-10T17:00:48.644002Z",
     "start_time": "2025-11-10T17:00:48.630209Z"
    }
   },
   "outputs": [],
   "execution_count": 26
  }
 ]
}
